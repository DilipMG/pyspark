{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c99c8d1",
   "metadata": {},
   "source": [
    "# Pyspark fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f926af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Spark Session is required to execute any code on a Spark Cluster. \n",
    "# It's also necessary for working with higher-level APIs like DataFrames and Spark SQL\n",
    "# It's not always mandatory though\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c9641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+\n",
      "|age|firstName|gender|lastName|    number|\n",
      "+---+---------+------+--------+----------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|\n",
      "| 32|    James|  male|   Smith|5678568567|\n",
      "| 24|    Emily|female|   Jones| 456754675|\n",
      "+---+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the json file people that is placed in same path as this notebook. Else specify full path\n",
    "df = spark.read.json('people.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c0e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema is used to print the schema of the dataframe. It displays the datatype and if the field is nullable. \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ddf2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'firstName', 'gender', 'lastName', 'number']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331f68f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, firstName: string, gender: string, lastName: string, number: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  describing the dataframe structure. describe() displays the datatype and field name\n",
    "# as seen age is string here\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f44264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+------+--------+--------------------+\n",
      "|summary| age|firstName|gender|lastName|              number|\n",
      "+-------+----+---------+------+--------+--------------------+\n",
      "|  count|   3|        3|     3|       3|                   3|\n",
      "|   mean|28.0|     null|  null|    null| 4.494868541333333E9|\n",
      "| stddev| 4.0|     null|  null|    null|3.5954963302739053E9|\n",
      "|    min|  24|    Emily|female| Jackson|           456754675|\n",
      "|    max|  32|      Joe|  male|   Smith|          7349282382|\n",
      "+-------+----+---------+------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe() along with show() displays count, mean, stddev, min and max details. \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364843c5",
   "metadata": {},
   "source": [
    "### Creating user defined schema with datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f77a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required methods from the package pyspark.sql.types\n",
    "from pyspark.sql.types import (StructType, StructField, \n",
    "                               IntegerType, StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b432db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a schema with the columns age, firstName, gender, lastName and number with required datatypes. \n",
    "# as seen age is defined as Integer now. \n",
    "\n",
    "# third argument True() says field can be nullable\n",
    "data_schema = [StructField (('age'), IntegerType(), True ),\n",
    "               StructField (('firstName'), StringType(), True ),\n",
    "               StructField (('gender'), StringType(), True ),\n",
    "               StructField (('lastName'), StringType(), True ),\n",
    "               StructField (('number'), StringType(), True )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa76fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the schema is required, create the Structure Type and assign the data_schema that is created. \n",
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf8d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the json file with the user defined file structure. \n",
    "df = spark.read.json('people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4bbd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema with userdefined structure. \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "047e5367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+\n",
      "|age|firstName|gender|lastName|    number|\n",
      "+---+---------+------+--------+----------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|\n",
      "| 32|    James|  male|   Smith|5678568567|\n",
      "| 24|    Emily|female|   Jones| 456754675|\n",
      "+---+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f6e87fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'age'>\n",
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "# df['age'] displays the Column but not exact data. \n",
    "print(df['age'])\n",
    "\n",
    "# When the type() is displayed, we can see its column.Column\n",
    "print(type(df['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d6a345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 28|\n",
      "| 32|\n",
      "| 24|\n",
      "+---+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# To display the data of any column, we need to use .select() function like SQL. \n",
    "df.select('age').show()\n",
    "\n",
    "# type() would display its dataframe.Dataframe\n",
    "print(type(df.select('age')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3874aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|gender|\n",
      "+---+------+\n",
      "| 28|  male|\n",
      "| 32|  male|\n",
      "| 24|female|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display multiple columns, specify the column names within select()\n",
    "df.select('age','gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aea5d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=28, firstName='Joe', gender='male', lastName='Jackson', number='7349282382'), Row(age=32, firstName='James', gender='male', lastName='Smith', number='5678568567')]\n",
      "------------------------------------\n",
      "Row(age=28, firstName='Joe', gender='male', lastName='Jackson', number='7349282382')\n",
      "------------------------------------\n",
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "# head() is used to Row data as RowData. \n",
    "# head(2) displays 2 records as list elements. \n",
    "# to select the 1st element from the list here, need to use the [0] and to select 2nd element, use [1], just like accessing list elements\n",
    "\n",
    "print(df.head(2))\n",
    "print('------------------------------------')\n",
    "print(df.head(2)[0])\n",
    "print('------------------------------------')\n",
    "print(type((df.head(2)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eba5bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+\n",
      "|age|firstName|gender|lastName|    number|\n",
      "+---+---------+------+--------+----------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|\n",
      "| 32|    James|  male|   Smith|5678568567|\n",
      "+---+---------+------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display first two entries in the dataframe use show(2)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63bc06be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+------+\n",
      "|age|firstName|gender|lastName|    number|newAge|\n",
      "+---+---------+------+--------+----------+------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|    56|\n",
      "| 32|    James|  male|   Smith|5678568567|    64|\n",
      "| 24|    Emily|female|   Jones| 456754675|    48|\n",
      "+---+---------+------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn adds new column to dataframe. We are adding 'newAge' here with value as twice of 'age'\n",
    "# !!!!!! Remember its temporary data unless it is assigned to some other dataFrame. !!!!!\n",
    "df.withColumn('newAge',df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4362f29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+\n",
      "|age|firstName|gender|lastName|    number|\n",
      "+---+---------+------+--------+----------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|\n",
      "| 32|    James|  male|   Smith|5678568567|\n",
      "| 24|    Emily|female|   Jones| 456754675|\n",
      "+---+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As seen below, the new column newAge is not really added to dataframe df. \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c8ff5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+------+\n",
      "|age|firstName|gender|lastName|    number|newAge|\n",
      "+---+---------+------+--------+----------+------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|    56|\n",
      "| 32|    James|  male|   Smith|5678568567|    64|\n",
      "| 24|    Emily|female|   Jones| 456754675|    48|\n",
      "+---+---------+------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assigning the df with new column to df_new. \n",
    "df_new = df.withColumn('newAge',df['age']*2).show()\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "530a0cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------+----------+\n",
      "|new_age|firstName|gender|lastName|    number|\n",
      "+-------+---------+------+--------+----------+\n",
      "|     28|      Joe|  male| Jackson|7349282382|\n",
      "|     32|    James|  male|   Smith|5678568567|\n",
      "|     24|    Emily|female|   Jones| 456754675|\n",
      "+-------+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed is used to rename the existing column. \n",
    "# !!!!!! Remember its temporary data unless it is assigned to some other dataFrame. !!!!!\n",
    "df.withColumnRenamed('age','new_age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caeebfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+\n",
      "|age|firstName|gender|lastName|    number|\n",
      "+---+---------+------+--------+----------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|\n",
      "| 32|    James|  male|   Smith|5678568567|\n",
      "| 24|    Emily|female|   Jones| 456754675|\n",
      "+---+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As seen below, the new column newAge is not really added to dataframe df.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68f8b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------+----------+\n",
      "|new_age|firstName|gender|lastName|    number|\n",
      "+-------+---------+------+--------+----------+\n",
      "|     28|      Joe|  male| Jackson|7349282382|\n",
      "|     32|    James|  male|   Smith|5678568567|\n",
      "|     24|    Emily|female|   Jones| 456754675|\n",
      "+-------+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assigning the df with new column to df_new. \n",
    "df_new1 = df.withColumnRenamed('age','new_age').show()\n",
    "df_new1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd7053",
   "metadata": {},
   "source": [
    "### Creating the Temporary View with the dataframe to access like the DB2/SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2ea4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the dataframe data to Temporary View names 'people'\n",
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56a91df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+\n",
      "|age|firstName|gender|lastName|    number|\n",
      "+---+---------+------+--------+----------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|\n",
      "| 32|    James|  male|   Smith|5678568567|\n",
      "| 24|    Emily|female|   Jones| 456754675|\n",
      "+---+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the data from 'people' can be queried now like SQL Queries using SELECT as seen below\n",
    "results = spark.sql(\"select * from people\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f4597c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------+----------+\n",
      "|age|firstName|gender|lastName|    number|\n",
      "+---+---------+------+--------+----------+\n",
      "| 28|      Joe|  male| Jackson|7349282382|\n",
      "| 32|    James|  male|   Smith|5678568567|\n",
      "+---+---------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query can contain the conditions as well just like regular SQL queries using WHERE clause\n",
    "results = spark.sql(\"select * from people where age > 25\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda489e",
   "metadata": {},
   "source": [
    "# Spark dataFrame Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95d8a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c86a826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read th csv file appl_stock into spark session\n",
    "df= spark.read.csv('appl_stock.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b815e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the top 10 rows of dataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "238d1645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the dataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93daafcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in the dataframe is : 1762\n",
      "The columns of the dataframe are          : ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
     ]
    }
   ],
   "source": [
    "# count() and columns() are used to get the count of the rows and also the column names of the dataframe. \n",
    "print(\"The number of records in the dataframe is :\", df.count())\n",
    "print(\"The columns of the dataframe are          :\",df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "497667ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n",
      "|      Date|     Open|            Close|\n",
      "+----------+---------+-----------------+\n",
      "|2014-06-13|92.199997|        91.279999|\n",
      "|2014-06-19|92.290001|        91.860001|\n",
      "|2014-06-20|91.849998|        90.910004|\n",
      "|2014-06-23|    91.32|90.83000200000001|\n",
      "|2014-06-24|    90.75|        90.279999|\n",
      "+----------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------+-----------------+\n",
      "|      Date|     Open|            Close|\n",
      "+----------+---------+-----------------+\n",
      "|2014-06-13|92.199997|        91.279999|\n",
      "|2014-06-19|92.290001|        91.860001|\n",
      "|2014-06-20|91.849998|        90.910004|\n",
      "|2014-06-23|    91.32|90.83000200000001|\n",
      "|2014-06-24|    90.75|        90.279999|\n",
      "+----------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write() also can be used similar to filter()\n",
    "df.where((df['Open']<100) & (df['Close']<92)).select(['Date','Open','Close']).show(5)\n",
    "\n",
    "# filter() also can be used similar to write()\n",
    "df.filter((df['Open']<100) & (df['Close']<92)).select(['Date','Open','Close']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "622ea4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just like SQL Where clause, we can use filter to select the records from dataframe based on condition. \n",
    "df.filter(\"Close < 500\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ff738cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Open|             Close|\n",
      "+----------+------------------+\n",
      "|213.429998|        214.009998|\n",
      "|214.599998|        214.379993|\n",
      "|214.379993|        210.969995|\n",
      "|    211.75|            210.58|\n",
      "|210.299994|211.98000499999998|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter() to select specific rows that has \"Close\" < 500. and select() to select specific columns. \n",
    "df.filter(\"Close < 500\").select([\"Open\", \"Close\"]).show(5)\n",
    "\n",
    "# df.filter(df['close']>500).select(['Open','Close']).show()     --> Even this can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "173312a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|              Open|     Close|\n",
      "+------------------+----------+\n",
      "|206.78000600000001|    197.75|\n",
      "|        204.930004|199.289995|\n",
      "|        201.079996|192.060003|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specifying Multiple conditions using filter(). \n",
    "df.filter((df['close'] < 200) & (df['Open'] > 200)).select(['Open','Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "469f7edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+------+------+---------+---------+\n",
      "|      Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+----------+------------------+----------+------+------+---------+---------+\n",
      "|2010-01-22|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+----------+------------------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 22), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Low'] == 197.16).show()    # --> show() would display the resultant like SQL output in tabluar format. \n",
    "\n",
    "df.filter(df['Low'] == 197.16).collect() # --> collect() would display the resultant in List/row format. Each data can then accessed like list elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "397f4143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2012, 9, 17), Open=699.349998, High=699.799995, Low=694.6100230000001, Close=699.7800219999999, Volume=99507800, Adj Close=91.052448),\n",
       " Row(Date=datetime.date(2012, 9, 18), Open=699.879997, High=702.329987, Low=696.4199980000001, Close=701.910004, Volume=93375800, Adj Close=91.329593),\n",
       " Row(Date=datetime.date(2012, 9, 19), Open=700.259979, High=703.989998, Low=699.569977, Close=702.100021, Volume=81718700, Adj Close=91.35431700000001),\n",
       " Row(Date=datetime.date(2012, 9, 20), Open=699.1599809999999, High=700.059975, Low=693.619987, Close=698.6999969999999, Volume=84142100, Adj Close=90.91192),\n",
       " Row(Date=datetime.date(2012, 9, 21), Open=702.409988, High=705.070023, Low=699.3599849999999, Close=700.089989, Volume=142897300, Adj Close=91.09278)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capturing the data that has 'Low' creater than 690. \n",
    "result = df.filter((df['Low'] > 690.0)).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77085372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99507800"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the above result, capturing the 1st occurence i.e., 0th element and then accessing \"Volume\" field\n",
    "row=result[0]\n",
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c8206",
   "metadata": {},
   "source": [
    "## Transforming and writing into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21a53ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "046c8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+---------+---------+\n",
      "|      Date|  Open|  High|   Low| Close|   Volume|Adj Close|\n",
      "+----------+------+------+------+------+---------+---------+\n",
      "|2010-01-04|213.43| 214.5|212.38|214.01|123432400|    27.73|\n",
      "|2010-01-05| 214.6|215.59|213.25|214.38|150476200|    27.77|\n",
      "|2010-01-06|214.38|215.23|210.75|210.97|138040000|    27.33|\n",
      "|2010-01-07|211.75| 212.0|209.05|210.58|119282800|    27.28|\n",
      "|2010-01-08| 210.3| 212.0|209.06|211.98|111902700|    27.46|\n",
      "+----------+------+------+------+------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "df=df.withColumn(\"Open\",round(col('Open'),2))\n",
    "df=df.withColumn(\"High\",round(col('High'),2))\n",
    "df=df.withColumn(\"Low\",round(col('Low'),2))\n",
    "df=df.withColumn(\"Close\",round(col('Close'),2))\n",
    "df=df.withColumn(\"Adj Close\",round(col('Adj Close'),2))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7b2639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+---------+---------+-------+\n",
      "|      Date|  Open|  High|   Low| Close|   Volume|Adj Close|Day_Avg|\n",
      "+----------+------+------+------+------+---------+---------+-------+\n",
      "|2010-01-04|213.43| 214.5|212.38|214.01|123432400|    27.73| 213.44|\n",
      "|2010-01-05| 214.6|215.59|213.25|214.38|150476200|    27.77| 214.42|\n",
      "|2010-01-06|214.38|215.23|210.75|210.97|138040000|    27.33| 212.99|\n",
      "|2010-01-07|211.75| 212.0|209.05|210.58|119282800|    27.28| 210.53|\n",
      "|2010-01-08| 210.3| 212.0|209.06|211.98|111902700|    27.46| 210.53|\n",
      "+----------+------+------+------+------+---------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding the average of the day\n",
    "df=df.withColumn(\"Day_Avg\", round(((col(\"High\") + col(\"Low\"))/2),2))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43e4da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+---------+---------+-------+----+\n",
      "|      Date|  Open|  High|   Low| Close|   Volume|Adj Close|Day_Avg|Year|\n",
      "+----------+------+------+------+------+---------+---------+-------+----+\n",
      "|2010-01-04|213.43| 214.5|212.38|214.01|123432400|    27.73| 213.44|2010|\n",
      "|2010-01-05| 214.6|215.59|213.25|214.38|150476200|    27.77| 214.42|2010|\n",
      "|2010-01-06|214.38|215.23|210.75|210.97|138040000|    27.33| 212.99|2010|\n",
      "|2010-01-07|211.75| 212.0|209.05|210.58|119282800|    27.28| 210.53|2010|\n",
      "|2010-01-08| 210.3| 212.0|209.06|211.98|111902700|    27.46| 210.53|2010|\n",
      "+----------+------+------+------+------+---------+---------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df=df.withColumn(\"Year\",year(df['Date']))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bba9c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\ndf.coalesce(1).write.csv(\\n    \"C:/Users/dilip/Documents/Dilip Studies/pyspark/updated\",\\n    header=True,\\n    mode=\"overwrite\"\\n)\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exporting the transformed dataframe --- WAS NOT SUCCESSFULL\n",
    "'''    \n",
    "df.coalesce(1).write.csv(\n",
    "    \"C:/Users/dilip/Documents/Dilip Studies/pyspark/updated\",\n",
    "    header=True,\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9033952",
   "metadata": {},
   "source": [
    "## GroupBy and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5488c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('aggs').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d41126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the csv file sales_info\n",
    "df=spark.read.csv(\"sales_info.csv\",inferSchema = True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a08e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb7d291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using groupby() on Company to find the aggregations. Here, finding the mean after grouping on Company column. \n",
    "df.groupBy(\"Company\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e65943dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding the sum of Sales per Company using groupBy() and sum()\n",
    "df.groupBy(\"Company\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f6cbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     130.0|\n",
      "|   GOOG|     120.0|\n",
      "|     FB|     350.0|\n",
      "|   MSFT|     124.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby() can also be used on other aggregations like count(), min(), max() etc\n",
    "df.groupBy(\"Company\").count().show()\n",
    "df.groupBy(\"Company\").min().show()\n",
    "df.groupBy(\"Company\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64edde4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|Company|Company_sales|\n",
      "+-------+-------------+\n",
      "|   APPL|       1480.0|\n",
      "|   GOOG|        660.0|\n",
      "|     FB|       1220.0|\n",
      "|   MSFT|        967.0|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy() and agg() can also be combined like below to get the desired result. here alias is used to give new column name\n",
    "from pyspark.sql.functions import sum \n",
    "\n",
    "df_new = df.groupBy('Company').agg(sum('Sales').alias('Company_sales'))\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c41983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|max(Sales)|\n",
      "+----------+\n",
      "|     870.0|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|min(Sales)|\n",
      "+----------+\n",
      "|     120.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg() can be used by specifying the parameters like key value pairs\n",
    "df.agg({'Sales':'sum'}).show()\n",
    "df.agg({'Sales':'max'}).show()\n",
    "df.agg({'Sales':'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f677d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|Company|count(Sales)|\n",
      "+-------+------------+\n",
      "|   APPL|           4|\n",
      "|   GOOG|           3|\n",
      "|     FB|           2|\n",
      "|   MSFT|           3|\n",
      "+-------+------------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     130.0|\n",
      "|   GOOG|     120.0|\n",
      "|     FB|     350.0|\n",
      "|   MSFT|     124.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg() can also be used on groupBy dataframe itself to find the aggregations on grouped data\n",
    "group_data = df.groupBy(\"Company\")\n",
    "group_data.agg({'Sales':'count'}).show()\n",
    "group_data.agg({'Sales':'max'}).show()\n",
    "group_data.agg({'Sales':'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a44b7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n",
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n",
      "+------------------+\n",
      "|stddev_samp(Sales)|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using other functions like Distinct count, Average and stad deviations.\n",
    "from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "df.select(countDistinct('Sales')).show()\n",
    "df.select(avg('Sales')).show()\n",
    "df.select(stddev('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ed00c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# format_number is used to rounding off the numberic data. In the below code, the Sales std dev is rounded off to 2 digits\n",
    "from pyspark.sql.functions import format_number\n",
    "sales_std = df.select(stddev('Sales').alias(\"Sales_Std_Dev\"))\n",
    "\n",
    "sales_std.select(format_number('Sales_Std_Dev',2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8c675dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0130685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataframe can be sorted using orderBy(), By default the data gets sorted in ascending order of the column specified\n",
    "df.orderBy(\"sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4715c82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To sort in descending, either of the below two code works. desc() or ascending=False would work \n",
    "df.orderBy(\"sales\",ascending=False).show()\n",
    "\n",
    "df.orderBy(df[\"sales\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98bfa2",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11d23389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName(\"Miss\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a727da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading the ContainsNull csv file. \n",
    "df = spark.read.csv(\"ContainsNull.csv\", header=True, inferSchema = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5389733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropping the rows that has Null value. na.drop() would drop all the rows that has even single null value. \n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b62accec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresh inside na.drop() is used to specify threshold. thresh=2 mean, drop the records with atleast 2 nulls in it. \n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00abc116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# na.drop(how='any') drops all the rows that has any null value in it. This is default one similar to na.drop()\n",
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db4379c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# na.drop(how='all') will only drop the rows if all the values are null. \n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d34cc742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset inside na.drop() used to check null values only in specific column\n",
    "df.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3c11881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc25a8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| null|\n",
      "|emp2|FILL VALUE| null|\n",
      "|emp3|FILL VALUE|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#na.fill() is used to fill the null values. By default, spark assigns the string fields if the value specified is string. \n",
    "df.na.fill('FILL VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7fbe7f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#na.fill() is used to fill the null values. By default, spark assigns the numeric fields if the value specified is numeric.\n",
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d69d7cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John|  0.0|\n",
      "|emp2|Missing|  0.0|\n",
      "|emp3|Missing|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Way to fill the missing values specific to columns\n",
    "df.na.fill({'Name':'Missing','Sales':0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4379298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n",
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best way is to specify the column in subset that needs to be filled and specify the data in na.fill()\n",
    "df.na.fill('No Name',subset=['Name']).show()\n",
    "df.na.fill('No Name',['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f821624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Sales)=400.5)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(Sales)|\n",
      "+----------+\n",
      "|     400.5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding the mean value using mean()\n",
    "from pyspark.sql.functions import mean\n",
    "mean_value = df.select(mean(df['Sales'])).collect()           # collect() takes the value and creates the result like List\n",
    "display(mean_value)\n",
    "\n",
    "mean_value1 = df.select(mean(df['Sales']))                    # without collect(), its a table data like query result\n",
    "mean_value1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42d33630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the data from row format\n",
    "mean_sales = mean_value[0][0]\n",
    "mean_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d721f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill null values in Sales column by mean value\n",
    "df.na.fill(mean_sales,['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20c5e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the above operation can be done like below in single line\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5e4ce",
   "metadata": {},
   "source": [
    "## Dates and Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e3a6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName(\"dates\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6e272ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading the appl_stock csv file into spark\n",
    "df=spark.read.csv(\"appl_stock.csv\",header=True, inferSchema = True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da47a858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 4), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039),\n",
       " Row(Date=datetime.date(2010, 1, 5), Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976000000002),\n",
       " Row(Date=datetime.date(2010, 1, 6), Open=214.379993, High=215.23, Low=210.750004, Close=210.969995, Volume=138040000, Adj Close=27.333178000000004),\n",
       " Row(Date=datetime.date(2010, 1, 7), Open=211.75, High=212.000006, Low=209.050005, Close=210.58, Volume=119282800, Adj Close=27.28265),\n",
       " Row(Date=datetime.date(2010, 1, 8), Open=210.299994, High=212.000006, Low=209.06000500000002, Close=211.98000499999998, Volume=111902700, Adj Close=27.464034)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "897e3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|              Open|\n",
      "+----------+------------------+\n",
      "|2010-01-04|        213.429998|\n",
      "|2010-01-05|        214.599998|\n",
      "|2010-01-06|        214.379993|\n",
      "|2010-01-07|            211.75|\n",
      "|2010-01-08|        210.299994|\n",
      "|2010-01-11|212.79999700000002|\n",
      "|2010-01-12|209.18999499999998|\n",
      "|2010-01-13|        207.870005|\n",
      "|2010-01-14|210.11000299999998|\n",
      "|2010-01-15|210.92999500000002|\n",
      "+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting 'data' and 'open' columns\n",
    "df.select('date','Open').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57bea8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the schema\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7367002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some of the functions of date and timestamp. \n",
    "from pyspark.sql.functions import (dayofmonth, hour, dayofyear, month, \n",
    "                                 year,weekofyear,format_number,date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2f3efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+----------+\n",
      "|dayofmonth(date)|month(date)|year(date)|\n",
      "+----------------+-----------+----------+\n",
      "|               4|          1|      2010|\n",
      "|               5|          1|      2010|\n",
      "|               6|          1|      2010|\n",
      "|               7|          1|      2010|\n",
      "|               8|          1|      2010|\n",
      "|              11|          1|      2010|\n",
      "|              12|          1|      2010|\n",
      "|              13|          1|      2010|\n",
      "|              14|          1|      2010|\n",
      "|              15|          1|      2010|\n",
      "|              19|          1|      2010|\n",
      "|              20|          1|      2010|\n",
      "|              21|          1|      2010|\n",
      "|              22|          1|      2010|\n",
      "|              25|          1|      2010|\n",
      "|              26|          1|      2010|\n",
      "|              27|          1|      2010|\n",
      "|              28|          1|      2010|\n",
      "|              29|          1|      2010|\n",
      "|               1|          2|      2010|\n",
      "+----------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To display the day of the months and month of the year\n",
    "df.select(dayofmonth(df['date']), month(df['date']), year(df['date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28a942f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|    210.58|119282800|          27.28265|2010|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copying the dataframe to new dataframe new_df and adding new column \"Year\" with year data from Date column\n",
    "new_df = df.withColumn(\"Year\",year(df['Date']))\n",
    "print(type(new_df))\n",
    "new_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f113f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2015|120.03999980555547|\n",
      "|2013| 472.6348802857143|\n",
      "|2014| 295.4023416507935|\n",
      "|2012| 576.0497195640002|\n",
      "|2016|104.60400786904763|\n",
      "|2010| 259.8424600000002|\n",
      "|2011|364.00432532142867|\n",
      "+----+------------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# grouping on Year to find the Mean of other columns & assigning it to new variable. \n",
    "result = new_df.groupBy(\"Year\").mean().select(['Year', 'avg(Close)'])\n",
    "result.show()\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b28e2eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|Year|Average Closing Price|\n",
      "+----+---------------------+\n",
      "|2015|   120.03999980555547|\n",
      "|2013|    472.6348802857143|\n",
      "|2014|    295.4023416507935|\n",
      "|2012|    576.0497195640002|\n",
      "|2016|   104.60400786904763|\n",
      "|2010|    259.8424600000002|\n",
      "|2011|   364.00432532142867|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming the column \"avg(Close)\" to \"Average Closing Price\".\n",
    "new = result.withColumnRenamed('avg(Close)','Average Closing Price')\n",
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce191e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|Avg Closing Price|\n",
      "+----+-----------------+\n",
      "|2015|           120.04|\n",
      "|2013|           472.63|\n",
      "|2014|           295.40|\n",
      "|2012|           576.05|\n",
      "|2016|           104.60|\n",
      "|2010|           259.84|\n",
      "|2011|           364.00|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# formatting the Avg Closing Price and rounding it off to 2 digits. \n",
    "new.select(['Year',format_number('Average Closing Price',2).alias('Avg Closing Price')]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
