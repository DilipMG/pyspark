{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb6d173",
   "metadata": {},
   "source": [
    "## Starting a simple spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d359a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510a5ea",
   "metadata": {},
   "source": [
    "## Loading the csv file to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de809bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"walmart_stock.csv\", header=True, inferSchema = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b43c4",
   "metadata": {},
   "source": [
    "## Count and column names of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc41cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in dataframe are: 1258\n",
      "The columns of the dataframe are      : ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of records in dataframe are:\",df.count())\n",
    "print (\"The columns of the dataframe are      :\",df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fee9fd",
   "metadata": {},
   "source": [
    "## Schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6f6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32adb5",
   "metadata": {},
   "source": [
    "## Displaying first 5 rows and 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af6b8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|\n",
      "+----------+------------------+---------+---------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0|\n",
      "|2012-01-09|         59.029999|59.549999|58.919998|             59.18|\n",
      "+----------+------------------+---------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cols = df.columns\n",
    "cols=df_cols[0:5]\n",
    "df.select(['Date','Open','High','Low','Close']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aadc902f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2012, 1, 3), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002),\n",
       " Row(Date=datetime.date(2012, 1, 4), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996),\n",
       " Row(Date=datetime.date(2012, 1, 5), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998),\n",
       " Row(Date=datetime.date(2012, 1, 6), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0),\n",
       " Row(Date=datetime.date(2012, 1, 9), Open=59.029999, High=59.549999, Low=58.919998, Close=59.18)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['Date','Open','High','Low','Close']).collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43b6ed",
   "metadata": {},
   "source": [
    "## Type casting to Integer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9b07a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Open: string, High: string, Low: string, Close: string, Volume: string, Adj Close: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eadbb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+--------+-------------+\n",
      "|summary|    Open|    High|     Low|   Close|       Volume|\n",
      "+-------+--------+--------+--------+--------+-------------+\n",
      "|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258.00|\n",
      "|   mean|   72.36|   72.84|   71.92|   72.39| 8,222,093.50|\n",
      "| stddev|    6.77|    6.77|    6.74|    6.76| 4,519,781.00|\n",
      "|    min|   56.39|   57.06|   56.30|   56.42| 2,094,900.00|\n",
      "|    max|   90.80|   90.97|   89.25|   90.47|80,898,096.00|\n",
      "+-------+--------+--------+--------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "result = df.describe()\n",
    "result\n",
    "\n",
    "result.select(result['summary'],\n",
    "             format_number(result['Open'].cast('float'),2).alias('Open'),\n",
    "             format_number(result['High'].cast('float'),2).alias('High'),\n",
    "             format_number(result['Low'].cast('float'),2).alias('Low'),\n",
    "             format_number(result['Close'].cast('float'),2).alias('Close'),\n",
    "             format_number(result['Volume'].cast('float'),2).alias('Volume')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37849567",
   "metadata": {},
   "source": [
    "## Creating new column HV Ratio that is ratio of High Price vs Volume of stock traded for a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2050d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_HV_Ratio=df.withColumn(\"HV Ratio\",df['High']/df['Volume'])\n",
    "df_HV_Ratio.select(\"HV Ratio\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c941c",
   "metadata": {},
   "source": [
    "## What day had the high Peak in price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6daa43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "|      Date|     Open|     High|  Low|    Close| Volume|Adj Close|\n",
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "|2015-01-13|90.800003|90.970001|88.93|89.309998|8215400|83.825448|\n",
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "[Row(Date=datetime.date(2015, 1, 13), Open=90.800003, High=90.970001, Low=88.93, Close=89.309998, Volume=8215400, Adj Close=83.825448)]\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "|      Date|     Open|     High|  Low|    Close| Volume|Adj Close|\n",
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "|2015-01-13|90.800003|90.970001|88.93|89.309998|8215400|83.825448|\n",
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.orderBy(df['High'].desc()).show(1))   # using orderBy and show\n",
    "print(df.orderBy(df['High'].desc()).head(1))   # using orderBy and head to display the rowset\n",
    "df.describe().show()                           # using describe() get the max High\n",
    "df.filter(\"High = 90.970001\").show()           # use the Max High value in the df.filter() functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650e2c1",
   "metadata": {},
   "source": [
    "## Mean of Close column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b35dcef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()                         # Using describe() we can get the mean data\n",
    "\n",
    "from pyspark.sql.functions import mean       # Other way is import mean function and use mean() for the column\n",
    "df.select(mean(\"Close\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20051191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "+-----------+\n",
      "|min(Volume)|\n",
      "+-----------+\n",
      "|    2094900|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(Volume)|\n",
      "+-----------+\n",
      "|   80898100|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## MAX and MIN of Volumne Column\n",
    "\n",
    "df.describe().show()\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "df.select(min('Volume')).show()\n",
    "df.select(max('Volume')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f69a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "81\n",
      "+------------+\n",
      "|count(Close)|\n",
      "+------------+\n",
      "|          81|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## How many days Close less than 60 dollars\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "print(df.filter('Close < 60').count())\n",
    "\n",
    "print(df.filter(df['Close']<60).count())\n",
    "\n",
    "result = df.filter(df['Close']<60)\n",
    "result.select(count('Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19cfa1c",
   "metadata": {},
   "source": [
    "## What % of High greater than 80 dollars\n",
    "## (Number of days > 80) / (Total days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bb656824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter('High > 80').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7e352882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f95f2abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.141494435612083"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = (df.filter('High > 80').count())/ (df.count())\n",
    "result*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b10b58",
   "metadata": {},
   "source": [
    "## Correlation between HIgh and Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9cf1cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| corr(High, Volume)|\n",
      "+-------------------+\n",
      "|-0.3384326061737161|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.select(corr('High','Volume')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd6a22",
   "metadata": {},
   "source": [
    "## Max high per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "413095f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+----+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|Year|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+----+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|2012|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|2012|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|2012|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|2012|\n",
      "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|2012|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "yearDf=df.withColumn(\"Year\",year(df['Date']))\n",
    "yearDf.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "692a42aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|max(High)|\n",
      "+----+---------+\n",
      "|2015|90.970001|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2012|77.599998|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df = yearDf.groupBy(\"Year\").max()\n",
    "max_df.select('Year','max(High)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9791a01",
   "metadata": {},
   "source": [
    "## Avg close for each month\n",
    "## Get the Avg Close value for Jan, Feb, Mar etc.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4cef9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+-----+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|Month|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+-----+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|    1|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|    1|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|    1|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|    1|\n",
      "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|    1|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "monthdf=df.withColumn(\"Month\",month(df['Date']))\n",
    "monthdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4bc91b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|       avg(Close)|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdf = monthdf.groupBy(\"Month\").avg()\n",
    "mdf.select('Month','avg(Close)').orderBy('Month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8460a3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Month|Avg(Close)|\n",
      "+-----+----------+\n",
      "|    1|     71.45|\n",
      "|    2|     71.31|\n",
      "|    3|     71.78|\n",
      "|    4|     72.97|\n",
      "|    5|     72.31|\n",
      "|    6|     72.50|\n",
      "|    7|     74.44|\n",
      "|    8|     73.03|\n",
      "|    9|     72.18|\n",
      "|   10|     71.58|\n",
      "|   11|     72.11|\n",
      "|   12|     72.85|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdf2 = monthdf.select(['Month','Close']).groupBy(\"Month\").mean()     ## Get the mean/average of the Monthwise data\n",
    "mdf2 = mdf2.withColumn('avg(Close)',format_number('avg(Close)',2))   ## Format the number to 2 digits\n",
    "mdf2.select('Month','Avg(Close)').orderBy('Month').show()            ## Select the columns to display with orderBy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
